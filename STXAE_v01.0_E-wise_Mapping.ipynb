{"cells":[{"cell_type":"markdown","metadata":{},"source":"# STXAE - Advanced structure tensor fiber analysis as basis for element wise orientation mapping onto finite elements\n*Author: Niels Jeppesen (<niejep@dtu.dk>)*, Robert Auenhammer (<robaue@chalmers.se>)\n\nIn this notebook we use Structure Tensor (ST) for orientation analysis of glass fiber composites. Those orientations are then mapped element-wise onto a finite element mesh. All types of elements, shells and solids, with all orders are possible. This notebook integrates the Structure Tensor method in an X-ray computer tomography aided engineering (XAE) process. \n\nThe ```structure-tensor``` package we will be using here is a 2D and 3D strcture tensor package for Python implemented by [Vedrana A. Dahl](mailto:vand@dtu.dk) and [Niels Jeppesen](mailto:niejep@dtu.dk)."},{"cell_type":"markdown","metadata":{},"source":"*****************************"},{"cell_type":"markdown","metadata":{},"source":"## Installation\nTo run this notebook there are some prerequisites that must be installed in our Python environment. We generally recommend [creating a new Python environment](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) to run the notebooks in, for instance using ```conda create -n <env_name> python=3.7```. This way we avoid conflicts with packages in your existing environment.\n\n### Option 1\nInstall dependencies (for this notebook) using ```pip install numpy scipy scikit-image matplotlib nibabel tqdm structure-tensor```. To start the notebook we will of course also need ```jupyter```.\n\n### Option 2\nRun ```pip install -r requirements.txt``` using the included ```requirements.txt``` file. **This will install specific versions of all packages needed to run this notebook, overwriting any current versions of those packages in our environment.** This also includes packages used in our other notebook.\n\nNow, let's go ahead and import the required modules. The ```structure_tensor_workers.py``` file should be in the notebook folder. "},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:Could not load CuPy for structure tensor analysis: No module named 'cupy'\n"]}],"source":"import os\nfrom multiprocessing import Pool\n\nimport matplotlib.font_manager as fm\nimport matplotlib.pyplot as plt\nimport nibabel as nib\nimport numpy as np\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom mpl_toolkits.axes_grid1.anchored_artists import AnchoredSizeBar\nfrom scipy import ndimage\nfrom scipy.spatial.transform import Rotation as R\nfrom skimage import filters\nfrom structure_tensor import eig_special_3d, structure_tensor_3d\nfrom tqdm import tqdm\n\nfrom structure_tensor_workers import calculate_angles, get_crops, init_worker, structure_tensor_analysis_v1\n\nimport pandas as pd\nimport csv\nimport numpy.f2py"},{"cell_type":"markdown","metadata":{},"source":"## Load and display data\nFirst, we'll load a sample of the data and some meta data. We will be using the following folder structure:\n- ```../notebooks``` contains the notebooks.\n- ```../originals``` should contain the original data files.\n- ```../tmp``` should contain any temperary files or output we create.\n- ```../notebooks/figures/<file_name>``` contains optionally saved figures."},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":"# Set file name and path.\nfile_name = 'HF401TT-13_FoV16.5_Stitch.txm_binning8.nii'\nfile_path = os.path.join('../data/', file_name)"},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":"# Change this to True to save figures to folder.\nsave_figures = False\n\n# Create folder for to save figures.\nfig_path = os.path.join('figures', os.path.basename(file_name))\nif save_figures and fig_path and not os.path.exists(fig_path):\n    os.makedirs(fig_path)"},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape: (74, 240, 528)\n","Data type: uint16\n","Voxel size: 64.62985229492188 μm\n","Fiber diameter size: 17 μm\n"]}],"source":"# Load data.\nnii_file = nib.load(file_path)\n\n# Read meta data.\ndata_shape = nii_file.shape\ndata_type = nii_file.get_data_dtype()\nvoxel_size = nii_file.affine[0, 0]\n# in micrometer\nfiber_diameter = 17\n\nprint('Shape:', data_shape)\nprint('Data type:', data_type)\nprint('Voxel size:', voxel_size, 'μm')\nprint('Fiber diameter size:', fiber_diameter, 'μm')"},{"cell_type":"markdown","metadata":{},"source":"   \nLet's have a need look at the data, to ensure it was loaded correctly."},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":"fig, axs = plt.subplots(1, 2, figsize=(20, 20))\ni = data_shape[-1] // 3\nimg = nii_file.dataobj[..., i]\naxs[0].imshow(img, cmap='gray')\ni = data_shape[-1] // 3 * 2\nimg = nii_file.dataobj[..., i]\naxs[1].imshow(img.squeeze(), cmap='gray')\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"## Crop data\nSome of the data may not of interest to us, so let's crop out the area of interest. You can cut out any part of the data you want to analyze."},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":"start_X=0\nend_X=nii_file.shape[0]\nstart_Y=0\nend_Y=nii_file.shape[1]\nstart_Z=0\nend_Z=nii_file.shape[2]\nx_slice = slice(start_Z, end_Z)\ny_slice = slice(start_Y, end_Y)\nz_slice = slice(start_X, end_X)"},{"cell_type":"markdown","metadata":{},"source":"Let's have a look at the cropped area. The TXM reader can be a bit slow, so we will only look at YZ-slices for now. We may have to come back and adjust the cropping on the X-axis later."},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":"fig, axs = plt.subplots(1, 2, figsize=(20, 20))\ni = data_shape[-1] // 3 * 2\nimg = nii_file.dataobj[z_slice, y_slice, i]\naxs[0].imshow(img, cmap='gray')\ni = data_shape[-1] // 3\nimg = nii_file.dataobj[z_slice, y_slice, i]\naxs[1].imshow(img, cmap='gray')\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"Create new shape. **Note: We rotate 90 degrees around the X axes to match convention.**"},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["New shape: (528, 240, 74)\n"]}],"source":"data_shape = (x_slice.stop - x_slice.start, y_slice.stop - y_slice.start, z_slice.stop - z_slice.start)\nprint('New shape:', data_shape)"},{"cell_type":"markdown","metadata":{},"source":"## Save as RAW\nTo work with the data, we will save it as a raw data file, which will allow us to access the data using ```numpy.memmap```. Using a memory-mapped file is both convenient and fast, especially since we will be accessing the data many times from different processes.\n\nWe will be placing all our processed data in a temporary folder, as it can be re-created using this notebook if deleted. We will also use data in the temp folder as a cache, so we don't have to create the RAW volume if it already exists. Thus, if you change the volume cropping etc., be sure to delete the RAW volume from the temp folder and create a new one."},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":"temp_folder = '../tmp/'\nif temp_folder and not os.path.exists(temp_folder):\n    os.mkdir(temp_folder)"},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 34.45it/s]"]},{"name":"stdout","output_type":"stream","text":["Creating new file: ../tmp/HF401TT-13_FoV16.5_Stitch.txm_binning8.nii_0-528_0-240_0-74.raw\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":"data_path = os.path.join(temp_folder, file_name + f'_{x_slice.start}-{x_slice.stop}_{y_slice.start}-{y_slice.stop}_{z_slice.start}-{z_slice.stop}.raw')\nif not os.path.exists(data_path):\n    \n    # Number of images read at a time.\n    # Reduce this to lower memory usage.\n    chunck_size = 1024\n    \n    print('Creating new file:', data_path)\n    data = np.memmap(data_path, dtype=data_type, shape=data_shape, mode='w+')\n    \n    for i in tqdm(range(0, data_shape[0], chunck_size)):\n        x0 = i + x_slice.start\n        x1 = min(x_slice.stop, x0 + chunck_size)\n        \n        chunck = nii_file.dataobj[z_slice, y_slice, x0:x1]\n        data[i:i + chunck_size] = np.rot90(chunck, k=3, axes=(0, 2))\n        \n\nelse:\n    print('Using existing file:', data_path)\ndata = np.memmap(data_path, dtype=data_type, shape=data_shape, mode='r')"},{"cell_type":"markdown","metadata":{},"source":"Let's have a look some slices from all three axes.\n\n**Important: Remember to look at figures below to verify that the correct data is loaded into the ```data``` variable. If you've changed the slicing or the data you're working with, you may want to delete the RAW data cached in the ```temp_folder``` folder.**"},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":"plt.figure(figsize=(20, 20))\nax = plt.subplot(2, 2, 1)\nax.imshow(data[data.shape[0] // 3], cmap='gray')\nax = plt.subplot(2, 2, 2)\nax.imshow(data[data.shape[0] // 3 * 2], cmap='gray')\nax = plt.subplot(2, 2, 3)\nax.imshow(data[:, data.shape[1] // 2], cmap='gray')\nax = plt.subplot(2, 2, 4)\nax.imshow(data[..., data.shape[-1] // 2], cmap='gray')\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"## Choosing fiber threshold\nTo choose a good intensity threshold for the fibers we can use a histogram and choose the value manually. Alternatively we can use a method such as Otsu's threshold. For choosing the threshold we make sure only to include inside-sample data. This makes it easier to separate foreground (fibers) and background inside the sample."},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 38 µs, sys: 4 µs, total: 42 µs\n","Wall time: 31.5 µs\n"]}],"source":"%%time\nthreshold_bins = 512\nthreshold_data = data[25:-25, 25:-25, 25]"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"otsu_threshold = filters.threshold_otsu(threshold_data.reshape(-1, 1), nbins=threshold_bins)\nhand_picked_threshold = 41000"},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Otsu threshold: 38164\n","Hand-picked threshold: 41000\n"]}],"source":"#This plot can be skipped if time is important\nax = plt.subplot(1, 1, 1)\nax.hist(threshold_data.flat, bins=threshold_bins, range=[5000, 60000], color=\"grey\",\n    edgecolor=\"1\",\n    linewidth=0.2)\nax.set_xlabel('Greyscale value', fontsize=15, fontweight ='bold')\nax.tick_params(axis='x', labelsize=13)\nax.tick_params(axis='y', labelsize=13)\nax.axvline(otsu_threshold, c='r')\nax.axvline(hand_picked_threshold, c='g')\nplt.show()\nprint('Otsu threshold:', otsu_threshold)\nprint('Hand-picked threshold:', hand_picked_threshold)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Otsu fiber fraction: 0.723\n","Hand-picked fiber fraction: 0.625\n"]}],"source":"print('Otsu fiber fraction:', round(np.count_nonzero(threshold_data >= otsu_threshold) / threshold_data.size, 3))\nprint('Hand-picked fiber fraction:', round(np.count_nonzero(threshold_data >= hand_picked_threshold) / threshold_data.size, 3))"},{"cell_type":"markdown","metadata":{},"source":"## Structure tensor\nWe set $\\sigma$ and $\\rho$ based on the size of the fibers that we want to analyze. $\\sigma$ and $\\rho$ are the only two parameters that need to be defined for this material orientation analysis. The value  For more details see the related paper and *StructureTensorFiberAnalysisDemo* notebook. \n\nThen, we set the block size (```crop_size```), which will determine the maximum size of the blocks we will partition the data into for the ST calculation. The maximum black size will depend on ```crop_size```, the ```truncate``` value used for Gaussian filters (default is 4), and $\\rho$ or $\\sigma$ (usually $\\rho$ because ti is largest).\n\nWe also set the ```fiber_threshold``` and specify a list of devices to use for the calculation. The list determines how the blocks will be distributed between devices for calculations. If we have a dual GPU system supporting CUDA, we can specify ```['cuda:0', 'cuda:1']```, which will distribute blocks evenly between the two GPUs. ```['cuda:0', 'cuda:1', 'cuda:1']``` would move two thirds to GPU 1, while ```['cuda:0', 'cuda:1', 'cpu']``` will move one third of the blocks to the CPU. **Remember to update the ```device``` list to match your system resources. Specifying CUDA devices that are not available will result in exceptions and/or undefined behaviour.** If you don't have CUDA GPUs available, just set ```device = ['cpu']``` and specify the number of processes later.\n\nThe ```class_vectors``` area used to segment the voxels based on the orientations of the ST eigenvectors. It is a list of unit vectors, which represent each of the fiber classes. The first vector is considered the primary orientation and will be used for calculating orientation metrics for each voxel. For more details see the related paper and *StructureTensorFiberAnalysisDemo* notebook.\n\nLastly, ```bins_per_degree``` determines the number of bins used per degree when aggregating the orientation metrics. "},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sigma: 0.6\n","rho: 1.5\n","crop_size: 334\n","Maximum block size: 340\n"]}],"source":"r = fiber_diameter / 2 / voxel_size\nsigma = round(np.sqrt(r**2 / 2), 2)\nrho = 4 * sigma\nsigma=0.6\nrho=1.5\n\nprint('sigma:', sigma)\nprint('rho:', rho)\n\ncrop_size = 334\nprint('crop_size:', crop_size)\nprint('Maximum block size:', crop_size + int(4 * max(rho, sigma) + 0.5))\n\nfiber_threshold = 41000\n# Important: Listing invalid CUDA devices may results in exceptions.\n#device = ['cuda'] \ndevice = ['cpu']\n\nclass_names = ['0', '45', '-45', '90']\nclass_vectors = np.array([[0, 0, 1], [0, 1, 1], [0, -1, 1], [0, 1, 0]], dtype=np.float64)\nclass_vectors /= np.linalg.norm(class_vectors, axis=-1)[..., np.newaxis]\n\nbins_per_degree = 4"},{"cell_type":"markdown","metadata":{},"source":"We can use the ```get_crops``` function to partition the data into blocks (```crops```) with proper padding. However, we will actually only use ```len(crops)``` here, as we will use ```multiprocessing``` to distribute the blocks accross multiple devices. We may include a function just for calculating the number of blocks/crops at a later point."},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"# Get crops as memory views.\ncrops, crop_positions, crop_paddings = get_crops(data, max(sigma, rho), crop_size=crop_size)"},{"cell_type":"markdown","metadata":{},"source":"We will be using the ```structure_tensor_analysis_v1``` function to calculate the structure tensor, $S$, and do the eigendecomposition for the blocks in parallel. We will be saving the following metrics to the disk using memory maps created below:\n\n1. ```vec_class``` contains a label for each voxel. \n2. ```eta_os``` contains a stiffness estimate, $\\eta_O$, for each voxel\n3. ```theta``` contains the angle, $\\theta$ between ```vec``` and class 0, at each voxel.\n4. ```out_xy_angle``` angle between```vec``` and the XY-plane (out-of-plane orientation).\n5. ```in_xy_angle``` rotation of ```vec``` in the XY-plane (in-plane orientation).\n\nTo calcualte these metrics, the ```structure_tensor_analysis_v1``` uses the ```calculate_angles``` function, which is explained further in the *StructureTensorFiberAnalysisDemo* notebook. However, the metrics returned by ```structure_tensor_analysis_v1``` have been aggregated (binned), as returning all the metrics for each block unaggregated would obvously consume large amounts of memory and be infeasible for large volumes. We will combine the aggregated data from each block afterwards.\n\nIn the code below we create memory-mapped files for the eigenvectors, along with three other metrics. The ```structure_tensor_analysis_v1``` function will be using these to save the metrics for the volume straight to the disk, which may require a significant amount of disk space, but shouldn't result in memory issues. Besides the five types shown below, the function also supports saving $S$ and the eigenvalues. See ```structure_tensor_workers.py``` for details. In the example below we will be saving the results using 16-bit precision data types to save space. This is usually fine for visualization and most statistics. Saving the metrics to disk is optional. **If you don't need the per voxel orientations for later, don't create the memory-maps and remove the entries from the ```init_args``` dictionary below.** This will save you a lot of disk space and probably reduce the processing time."},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating memory mapped file:\n","../tmp/HF401TT-13_FoV16.5_Stitch.txm_binning8.nii_0-528_0-240_0-74-vec-0.6-1.5.raw\n","Creating memory mapped file:\n","../tmp/HF401TT-13_FoV16.5_Stitch.txm_binning8.nii_0-528_0-240_0-74-vec_class-0.6-1.5-41000.raw\n","Creating memory mapped file:\n","../tmp/HF401TT-13_FoV16.5_Stitch.txm_binning8.nii_0-528_0-240_0-74-theta-0.6-1.5-41000.raw\n","Creating memory mapped file:\n","../tmp/HF401TT-13_FoV16.5_Stitch.txm_binning8.nii_0-528_0-240_0-74-in_xy_angle-0.6-1.5-41000.raw\n","Creating memory mapped file:\n","../tmp/HF401TT-13_FoV16.5_Stitch.txm_binning8.nii_0-528_0-240_0-74-out_xy_angle-0.6-1.5-41000.raw\n"]}],"source":"# Output names, dtypes and shapes.\nmap_names = ['vec', 'vec_class', 'theta', 'in_xy_angle', 'out_xy_angle']\nmap_dtypes = [np.float16, np.uint8, np.float16, np.float16, np.float16]\nmap_shapes = [(3, ) + data.shape, data.shape, data.shape, data.shape, data.shape]\n\n# Output paths.\nmap_paths = {n: data_path.replace('.raw', f'-{n}-{sigma}-{rho}-{fiber_threshold}.raw') for n in map_names}\nmap_paths['vec'] = data_path.replace('.raw', f'-vec-{sigma}-{rho}.raw')\n\n# Create maps.\nmaps = {}\nnew_maps = {}\n\nfor n, dtype, shape in zip(map_names, map_dtypes, map_shapes):\n    path = map_paths[n]\n    create_map = not os.path.exists(path) or os.stat(path).st_size != np.product(shape) * np.dtype(dtype).itemsize\n    \n    if create_map:\n        print(f'Creating memory mapped file:\\n{path}')\n        mmap = np.memmap(path, dtype=dtype, shape=shape, mode='w+')\n        if np.issubdtype(dtype, np.floating) and n not in ['S', 'val', 'vec']:\n            mmap[:] = np.nan\n            \n        new_maps[n] = (path, dtype)\n        \n    maps[n] = np.memmap(path, dtype=dtype, shape=shape, mode='r')"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":"# Get memory maps.\nvec = maps['vec']\nvec_class = maps['vec_class']\ntheta = maps['theta']\nin_xy_angle = maps['in_xy_angle']\nout_xy_angle = maps['out_xy_angle']"},{"cell_type":"markdown","metadata":{},"source":"Now we're finally ready to perform the analysis. We will be using ```multiprocessing.Pool``` to distribute the work across multiple CPU cores. If specified in the ```device``` list, the CPU will offload the majority of the work to a GPU, otherwise it'll do the calculations itself. Here, we will create four processes for each device in the list (```processes=4 * len(device)```). As our ```device``` list contains four GPUs, we will be starting 16 processes, four for each GPU. Beware that this may require a significant amount of GPU memory. **If you experience out of memory exceptions, either reduce ```crop_size``` and/or the number of processes per device.** Change the number of processes to fit your system. "},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n"]}],"source":"if __name__ ==  '__main__':\n    results = []\n    \n    init_args = {'data': data_path,\n                 'dtype': data.dtype,\n                 'shape': data.shape,\n                 'rho': rho,\n                 'sigma': sigma,\n                 'crop_size': crop_size,\n                 'threshold': fiber_threshold,\n                 'class_vectors': class_vectors,\n                 'bins_per_degree': bins_per_degree,\n                 'device': device\n                }\n    \n    # Add output maps to args.\n    for n in new_maps:\n        path, dtype = new_maps[n]\n        init_args[n] = path\n        init_args[f'{n}_dtype'] = dtype\n\n    with Pool(processes=4 * len(device), initializer=init_worker, initargs=(init_args,)) as pool:\n        for res in tqdm(pool.imap_unordered(structure_tensor_analysis_v1, range(len(crops)), chunksize=1), total=len(crops)):\n            results.append(res)"},{"cell_type":"markdown","metadata":{},"source":"All shared variables are passed to the workers using the ```init_args``` dictionary and ```init_worker```function. This function is only called once per worker. When called, it will create all the memory-maps needed to read and write data during calculations, based on ```init_args```. We will use the ```pool.imap_unordered``` function to tell the workers to perform the calculations for a block, by passing the ```structure_tensor_analysis_v1``` function along with the index of the block which we want to analyze. The returned results are a tuple of aggregated metrics, as decribed earlier. For more details see the ```structure_tensor_analysis_v1``` code. Below we will show have to combine the aggregated metrics and display them as histograms.\n\nAnother option is to save all the metrics to disk (using memory-maps as described earlier) with reasonable precision and perform the statistics directly on the full orientation volumes. This approach would be similar to what's done in the *StructureTensorFiberAnalysisDemo* notebook, except there data is kept in memory instead of on the disk. If you use this approach you may simply ignore the aggregated data returned by ```structure_tensor_analysis_v1```, but it will require you to use a significant amount of disk space to store all the orientation and segmentation data. If the volumes are very big, working with them may also be slow and memory intensive, even if you use ```memmap``` to access the data."},{"cell_type":"markdown","metadata":{},"source":"## Validation Structure Tensor outcome\nThe following cells serve as validation possiblity for the outcome of the calculate structure tensor above. Those cells do not have to be run for the mapping later on."},{"cell_type":"markdown","metadata":{},"source":"******"},{"cell_type":"markdown","metadata":{},"source":"## Histograms\nThe figures above are great for validating that we've done the calculations correctly and that our results are meaningful. However, even with this many figures we're only actually showing a very small fraction of the volume. To get a proper overview of the fiber orientations and classes we need to aggregate the information in a meaningful way. One way to do this is to use histograms. Since we have saved all the data to the memory maps, we just need to aggregate the data that they contain.\n\n**Note: To load the memory maps from disk later, simply use ```np.memmap```, for example ```theta = np.memmap(path, dtype=dtype, shape=shape, mode='r')```.**\n\nBy combining the threshold mask (indicating what is fiber) with the class labels, we can create a fiber class mask for each class."},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":"# Create threshold mask.\nthreshold_mask = data >= fiber_threshold\n# Count number of fiber voxels.\nfiber_count = np.count_nonzero(threshold_mask)\n# Create a mask for each fiber class.\nvec_class_masks = [threshold_mask] + [(vec_class == l) & threshold_mask for l in range(len(class_vectors))]"},{"cell_type":"markdown","metadata":{},"source":"### Fiber fractions\nUsing these masks we calculate the fraction of the fibers belonging to each class."},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fraction (0): 0.734655\n","Fraction (45): 0.109877\n","Fraction (-45): 0.118585\n","Fraction (90): 0.036883\n"]}],"source":"fiber_fractions = [np.count_nonzero(m) / fiber_count for m in vec_class_masks[1:]]\nfor i, f in enumerate(fiber_fractions):\n    c = class_names[i]\n    print(f'Fraction ({c}): {round(f, 6)}')"},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":"plt.bar(class_names, fiber_fractions, align='center', width=0.5, color=\"grey\")\nplt.xticks(range(len(fiber_fractions)))\nplt.xlabel('Class', fontsize=15, fontweight ='bold')\nplt.ylabel('Fraction', fontsize=15, fontweight ='bold')\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"### Angle from X ($\\theta$)\nThe histograms below show the distribution of the absolute angles between the fibers and the X-axis unit vector, along with the median angle. The resoulution of the histograms is determined by ```bins_per_degree```."},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":"# Four bins per degree = 0.25 degree resolution.\nbins_per_degree = 4"},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 5/5 [00:02<00:00,  2.05it/s]\n"]}],"source":"def hist(i, ax):\n    # Calculate theta for each class using the masks.\n    values = theta[vec_class_masks[i]]\n    values = np.array(values[~np.isnan(values)], dtype=np.float32)\n    \n    if i == 0:\n        title = 'All classes'\n        ax.set_xlim([0, 90])\n    else:\n        class_name = class_names[i - 1]\n        title = f'Class {class_name}'\n        class_value = int(class_name)\n        ax.set_xlim([max(0, abs(class_value) - 30) , min(90, abs(class_value) + 30)])\n    \n        m = np.median(values).item()\n        ax.axvline(m, c='k', ls='-', label=f'$Med={round(m, 2)}\\degree$')\n        plt.legend(prop={\"size\":14})\n\n    ax.set_title(title, fontsize=18)\n    ax.hist(values, density=True, bins=np.arange(0, 90 + 1 / bins_per_degree, 1 / bins_per_degree), color=\"grey\",\n    edgecolor=\"1\",\n    linewidth=0.7)\n    ax.set_xlabel('Angle [$\\degree$]', fontsize=15, fontweight ='bold')\n    ax.set_ylabel('Fraction', fontsize=15, fontweight ='bold')\n    ax.tick_params(axis='x', labelsize=13)\n    ax.tick_params(axis='y', labelsize=13)\n\nfig = plt.figure(figsize=(15, 10))\nfig.suptitle('Angle from X', fontsize=22)\n\nfor i in tqdm(range(len(class_names) + 1)):\n    ax = plt.subplot(2, 3, i + 1)\n    hist(i, ax)\n    \nplt.tight_layout()\nplt.subplots_adjust(top=0.85)  \nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"### In-plane orientation (rotaiton in XY-plane)\nThe histograms below show the distribution of the rotation around the Z-axis (in-XY-plane rotation), the mean and the standard deviation for each class."},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 5/5 [00:03<00:00,  1.39it/s]\n"]}],"source":"def hist(i, ax):\n    values = in_xy_angle[vec_class_masks[i]]\n    values = np.array(values[~np.isnan(values)], dtype=np.float32)\n    \n    if i == 0:\n        title = 'All classes'\n        class_value = 0\n        ax.set_xlim([-90, 90])\n        \n        ax.axvline(0, c='k', ls='--')\n        ax.axvline(-45, c='k', ls='--')\n        ax.axvline(45, c='k', ls='--')\n    else:\n        class_name = class_names[i - 1]\n        title = f'Class {class_name}'\n        class_value = int(class_name)\n        \n        ax.set_xlim([class_value - 20, class_value + 20])\n#        ax.axvline(class_value, c='k', ls='--', label=f'${class_value}\\degree$')\n\n    if class_value == 90:\n        ax.set_xticks(range(class_value - 30, class_value + 31, 10))\n        ax.set_xticklabels([60, 70, 80, '$\\pm90$', -80, -70, -60])\n        bins = np.arange(0, 180 + 1 / bins_per_degree, 1 / bins_per_degree)\n        values[values < 0] = 90 + (values[values < 0] + 90)\n    else:\n        bins = np.arange(-90, 90 + 1 / bins_per_degree, 1 / bins_per_degree)\n    \n    ax.set_title(title, fontsize=18)\n    ax.hist(values, density=True, bins=bins, color=\"grey\",\n    edgecolor=\"1\",\n    linewidth=0.5)\n\n    ax.set_xlabel('Angle [$\\degree$]', fontsize=15, fontweight ='bold')\n    ax.set_ylabel('Fraction', fontsize=15, fontweight ='bold')\n    ax.tick_params(axis='x', labelsize=13)\n    ax.tick_params(axis='y', labelsize=13)\n#    ax.set_ylim([0, 0.2])\n    mean = np.mean(values).item()\n    std = np.std(values).item()\n    ax.set_xlabel('Angle [$\\degree$]')\n    ax.set_ylabel('Fraction')\n    if i != 0:\n        ax.axvline(mean, c='k', ls='-', label=f'$\\\\bar{{x}}={round(mean if mean < 90 else -180 + mean, 2)}\\degree$')\n        ax.axvline(mean - std, c='k', ls='--', label=f'$s=\\pm{round(std, 2)}\\degree$')\n        ax.axvline(mean + std, c='k', ls='--')\n        plt.legend(prop={\"size\":14})\n\nfig = plt.figure(figsize=(15, 10))\nfig.suptitle('In-plane orientation', fontsize=22)\n    \nfor i in tqdm(range(len(class_names) + 1)):\n    ax = plt.subplot(2, 3, i + 1)\n    hist(i, ax)\n    \nplt.tight_layout()\nplt.subplots_adjust(top=0.85)\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"### Out-of-plane orientation (angle from xy-plane)\nThe histograms below show the distribution of the angle from the out-of-plane (XY-plane) angle."},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":"def hist(i, ax):\n    values = out_xy_angle[vec_class_masks[i]]\n    values = np.array(values[~np.isnan(values)], dtype=np.float32)\n    \n    if i == 0:\n        title = 'All classes'\n        class_value = None\n    else:\n        title = f'Class {class_names[i - 1]}'\n        class_value = int(class_names[i - 1])\n\n    ax.set_title(title, fontsize=18)\n    \n    mean = np.mean(values).item()\n    std = np.std(values).item()\n    \n    if class_value == 90:\n        ax.set_xlim([-20, 20])\n    else:\n        ax.set_xlim([-10, 10])\n    ax.hist(values, density=True, bins=np.arange(-90, 90 + 1 / bins_per_degree, 1 / bins_per_degree), color=\"grey\",\n    edgecolor=\"1\",\n    linewidth=0.75)\n    ax.set_xlabel('Angle [$\\degree$]', fontsize=15, fontweight ='bold')\n    ax.set_ylabel('Fraction', fontsize=15, fontweight ='bold')\n    ax.tick_params(axis='x', labelsize=13)\n    ax.tick_params(axis='y', labelsize=13)\n#    ax.set_ylim([0, 0.3])\n\n#    ax.axvline(0, c='k', ls='--', label='$0\\degree$')\n    if i != 0:\n        ax.axvline(mean, c='k', ls='-', label=f'$\\\\bar{{x}}={round(mean, 2)}\\degree$')\n        ax.axvline(mean - std, c='k', ls='--', label=f'$s=\\pm{round(std, 2)}\\degree$')\n        ax.axvline(mean + std, c='k', ls='--')\n        plt.legend(prop={\"size\":14})\n\nfig = plt.figure(figsize=(15, 10))\nfig.suptitle('Out-of-plane orientation', fontsize=22)\n\nfor i in range(len(class_names) + 1):\n    ax = plt.subplot(2, 3, i + 1)\n    hist(i, ax)\n    \nplt.tight_layout()\nplt.subplots_adjust(top=0.85)    \nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"The class fractions and orientation distributions are useful for determining if the material follows production specifications. Here we've chosen a specific set of metrics to calculate and plot, but many other distributions can just as easily be obtained and plotted."},{"cell_type":"markdown","metadata":{},"source":"## Slices\nAs we chose to save the eigenvectors for each voxel in the ```data``` volume to the disk using the ```vec``` memory map, we can actually access all the eigenvectors along with the original data if we like. We also have orientation metrics avilable through the ```theta```, ```in_xy_angle```, ```out_xy_angle``` memory-maps, but they were saved with low precision intended only for visualization in other applications and we will not be using these data in this notebook.\n\nLet's grab som data and their ST eigenvectors. The vectors were saved as ```float16``` to save space, but before we can plot them we have to change their type. Generally, CPUs do not support ```float16``` natively, so using this data type for anything but storing large data sets is generally not advisable.\n\n**Note: We've chosen some specific slices from our data, such as Z-index 40 and 80. If you Z-dimension size is 80 or less this will give an error. You can easily change which slices will be shown in the figures below, by changing/adding/removing the ```data_slices``` and ```vec_slices``` indices.** "},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":"data_slices = [\n    data[data.shape[0] // 2].copy(),\n    data[:, data.shape[1] // 2].copy(),\n    data[:, :, data.shape[2] // 2].copy(),\n    data[:, :, data.shape[2] // 2].copy(),\n]\n\nvec_slices = [\n    vec[:, vec.shape[1] // 2].astype(np.float32),\n    vec[:, :, vec.shape[2] // 2].astype(np.float32),\n    vec[:, :, :, vec.shape[3] // 2].astype(np.float32),\n    vec[:, :, :, vec.shape[3] // 3].astype(np.float32),\n]"},{"cell_type":"markdown","metadata":{},"source":"Let's can have a look at the data and the first element of the eigenvectors."},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":"fig, axs = plt.subplots(1, len(data_slices), figsize=(20, 20))\nfor img, ax in zip(data_slices, axs):\n    ax.imshow(img, cmap='gray')\nplt.show()\n\nfig, axs = plt.subplots(1, len(vec_slices), figsize=(20, 20))\nfor img, ax in zip(vec_slices, axs):\n    ax.imshow(img[0])\nplt.show()"},{"cell_type":"markdown","metadata":{},"source":"## Slices with angle information\nJust as we did in *StructureTensorFiberAnalysisDemo*, we can use the ```calculate_angles``` function to calculate the segmentation and orientation metrics for the slices we just read from the volumes.\n\nFirst we define some plotting functions, then we calculate ```vec_class```, ```eta_os```, ```theta```, ```out_xy_angle```  and ```in_xy_angle``` for each of the slices and plot them. All the figures will be saved to the figures folder we created in the beginning. It easy to create figures for other slices simply be changing which slices are extracted from the volumes."},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":"def add_scalebar(ax, length=100, scale=1, unit='μm', text=None):\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    text = f'{length} {unit}' if text is None else text\n    \n    fontprops = fm.FontProperties(size=18)\n    scalebar = AnchoredSizeBar(ax.transData,\n                               length / scale, text, 'lower left', \n                               pad=0.3,\n                               color='white',\n                               frameon=False,\n                               size_vertical=3,\n                               fontproperties=fontprops)\n    ax.add_artist(scalebar)\n    \ndef savefig(ax, i):\n    title = ax.get_title()\n    ax.set_title(None)\n    plt.savefig(os.path.join(fig_path, f'{title}_{i}.svg'), bbox_inches='tight', pad_inches=0, dpi=300)\n    plt.savefig(os.path.join(fig_path, f'{title}_{i}.pdf'), bbox_inches='tight', pad_inches=0, dpi=300)\n    ax.set_title(title)"},{"cell_type":"markdown","metadata":{},"source":"**NOTE: You can easily change which figures are made by commenting out the calls to ```fig_with_colorbar``` in the ```show_metrics``` function.** "},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_41/2208991995.py:5: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n","  fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n"]}],"source":"def fig_with_colorbar(i, d, o, title, alpha=0.5, cmap=None, vmin=None, vmax=None):\n    \"\"\"Creates a figure with data, overlay and color bar.\"\"\"\n    o = np.rot90(o, k=-1)\n    d = np.rot90(d, k=-1)\n    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes('right', size='3%', pad=0.05)\n    ax.imshow(d, cmap='gray')\n    if np.issubdtype(o.dtype, np.integer):\n        cmap = plt.get_cmap('gist_rainbow', len(class_names))\n        im = ax.imshow(o, alpha=alpha, cmap=cmap, vmin=-.5, vmax=len(class_names) - .5)\n        cbar = fig.colorbar(im, cax=cax, orientation='vertical', ticks=np.arange(len(class_names)))\n        cbar.ax.set_yticklabels(class_names)\n    else:\n        im = ax.imshow(o, alpha=alpha, cmap=cmap, vmin=vmin, vmax=vmax)\n        fig.colorbar(im, cax=cax, orientation='vertical')\n    ax.set_title(title)\n    add_scalebar(ax, length=2000, scale=voxel_size, text='2 mm')\n    if save_figures:\n        savefig(ax, i)\n    plt.show()\n\ndef show_metrics(data_slices, vec_slices, fiber_threshold=None):\n    \n    for i, (d, v) in enumerate(zip(data_slices, vec_slices)):\n        vec_class, eta_os, theta, out_xy_angle, in_xy_angle = calculate_angles(v, class_vectors)\n\n        if fiber_threshold is not None:\n            fiber_mask = d <= fiber_threshold\n\n            vec_class = np.ma.masked_where(fiber_mask, vec_class)          \n            eta_os[fiber_mask] = np.nan\n            theta[fiber_mask] = np.nan\n            out_xy_angle[fiber_mask] = np.nan\n            in_xy_angle[fiber_mask] = np.nan\n\n        fig_with_colorbar(i, d, vec_class, 'Class', alpha=0.7)\n        fig_with_colorbar(i, d, eta_os, '$\\eta_O$(X)', alpha=0.5, vmin=0, vmax=1)\n        fig_with_colorbar(i, d, theta, 'Angle from X (0-90 deg.)', alpha=0.5, vmin=0, vmax=90)\n        fig_with_colorbar(i, d, in_xy_angle, 'In-plane orientation (-90-90 deg.)', cmap='bwr', alpha=0.5, vmin=-90, vmax=90)\n        fig_with_colorbar(i, d, out_xy_angle, 'Out-of-plane orientation (-90-90 deg.)', cmap='bwr', alpha=0.5, vmin=-90, vmax=90)\n        fig_with_colorbar(i, d, theta, 'Angle from X (0-10 deg.)', alpha=0.5, vmin=0, vmax=10)\n        fig_with_colorbar(i, d, in_xy_angle, 'In-plane orientation (-10-10 deg.)', cmap='bwr', alpha=0.5, vmin=-10, vmax=10)\n        fig_with_colorbar(i, d, out_xy_angle, 'Out-of-plane orientation (-10-10 deg.)', cmap='bwr', alpha=0.5, vmin=-10, vmax=10)\n        \nshow_metrics(data_slices, vec_slices, fiber_threshold=fiber_threshold)"},{"cell_type":"markdown","metadata":{},"source":"**********************"},{"cell_type":"markdown","metadata":{},"source":"## Material orientation mapping\n\nFor the material orientation mapping for the integration point-wise option an Abaqus .inp-file must be supplied. The mapping output is supplied via a new .inp-file containing of the original .inp-file and the appened material orientation in an executable Abaqus syntax. Furthermore, element sets for Abaqus are output."},{"cell_type":"markdown","metadata":{},"source":"Opens the .inp-file from the simulation and finds the lines where parts can be removed"},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Coordinate information of the nodes lies within lines 34 and 135499\n","Element definition lies within lines 135504 and 791019\n"]}],"source":"fn = '../data/bundle_X002.inp'\nfile1 = open(fn,\"r\") \n\n\nn_N_start=0\nn_N_end=0\nn_E_start=0\nn_E_end=0\nrest=0\nfor line in file1:\n    n_N_start=n_N_start+1\n    if \"*NODE\" in line:\n        Start_Nodes=n_N_start+1\n        n_N_end=n_N_start\n        for line in file1:\n            n_N_end=n_N_end+1\n            if \"**\" in line:\n                End_Nodes=n_N_end-1\n                for line in file1:\n                    n_E_start=n_E_start+1\n                    if \"*ELEMENT\" in line:\n                        Start_Elements=n_E_start+n_N_end+1\n                        for line in file1:\n                            n_E_end=n_E_end+1\n                            if \"**\" in line:\n                                End_Elements=n_E_end+n_E_start+n_N_end-1\n                                for line in file1:\n                                    rest=rest+1\n                                break\n                        break\n                break\n        break\nSF_nodes=rest+n_E_end+n_E_start+n_N_end-End_Nodes\nSF_elements=rest+n_E_end+n_E_start+n_N_end-End_Elements\nprint('Coordinate information of the nodes lies within lines', Start_Nodes, 'and', End_Nodes)\nprint('Element definition lies within lines', Start_Elements, 'and', End_Elements)"},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":"file_n1 = pd.read_csv(fn, skiprows=Start_Nodes-1, skipfooter=SF_nodes, header=None, engine='python')\nfile_e1 = pd.read_csv(fn, skiprows=Start_Elements-1, skipfooter=SF_elements, header=None, engine='python')\nfile_n1.to_csv('../results/File_nodes.csv',index=False)\nfile_n2= open('../results/File_nodes.csv', newline='')\nfile_e1.to_csv('../results/File_elements.csv',index=False)\nfile_e2= open('../results/File_elements.csv', newline='')"},{"cell_type":"markdown","metadata":{},"source":"Creates a numpy array for all nodes and elements"},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Node array created\n","Element array created\n"]}],"source":"# Creates a numpy array for the coordinates of every single node\n#\nnodes = csv.reader(file_n2)\nheader = next(nodes)\n\nMax_no_Nd=np.shape(file_n1)[0]\n\nnodes_arr=np.empty((Max_no_Nd,4))\nfor row in nodes:\n    i=int(row[0])-1\n    for j in range(4):\n        if j==0:\n            nodes_arr[i][j] = int(row[j])\n        else:\n            nodes_arr[i][j] = float(row[j])            \nprint('Node array created')\n#\n#\nelements = csv.reader(file_e2)\nheader = next(elements)\n\nNo_Nodes_per_El=np.shape(file_e1)[1]-1\nMax_no_El=np.shape(file_e1)[0]\n\nelements_arr=np.empty((Max_no_El,No_Nodes_per_El+1))\nfor row in elements:\n    k=int(row[0])-1\n    for l in range(No_Nodes_per_El+1):\n        elements_arr[k][l] = int(row[l])\nelements_arr=elements_arr.astype(int)   \nprint('Element array created')"},{"cell_type":"markdown","metadata":{},"source":"Here the position of the image data and finite element mesh get aligned. It is designed that the finite element mesh was based directly on the image data. If the finite element mesh is created independently adjustments might be necessary."},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["X-origin: 0.0\n","Y-origin: 0.0\n","Z-origin: 0.0\n"]}],"source":"X0=start_X*voxel_size\nY0=start_Y*voxel_size\nZ0=start_Z*voxel_size\nprint('X-origin:', X0)\nprint('Y-origin:', Y0)\nprint('Z-origin:', Z0)"},{"cell_type":"markdown","metadata":{},"source":"# Mapping \n\nHere the mapping of the structure tensor outcome onto the single finite elements happens. Note that g3 and g1 are switched since the structure tensor vec has the order z, y, x. The data is output as an appendix to the original .inp-file in a new .inp-file. Additionally, element sets for the different bundle types based on the orientations are output."},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":"# Mapping\n#\n#---------------------------------------------------------------------------\nfile1 = open(\"../data/bundle_X002.inp\",\"r\") \noutput = []\nfor line in file1:\n    if line.startswith('*SOLID SECTION'):\n        output.append(f'{line[:-1]}' ', ORIENTATION=DISTRIBUTION_MAT_ORIENTATION_1'\"\\n\")\n    elif line.startswith('*Solid Section'):\n        output.append(f'{line[:-1]}' ', ORIENTATION=DISTRIBUTION_MAT_ORIENTATION_1'\"\\n\")\n    else:\n        output.append(line)\nfile1.close()\nfn_new='../results/bundle_Y001_orient.inp'\nf = open(fn_new, 'w')\nf.writelines(output)\nf.close()\n\nel_uni = []\nel_backing = []\nel_biax = []\ncogx_sum = np.empty(No_Nodes_per_El)\ncogy_sum = np.empty(No_Nodes_per_El)\ncogz_sum = np.empty(No_Nodes_per_El)\ncogx     = np.empty(Max_no_El)\ncogy     = np.empty(Max_no_El)\ncogz     = np.empty(Max_no_El)\nVecOrientX=np.empty(Max_no_El)\nVecOrientY=np.empty(Max_no_El)\nVecOrientZ=np.empty(Max_no_El)\nwith open(fn_new,'a') as output:\n    output.write('**DISTRIBUTION_MAT_ORIENTATION' '\\n')\n    output.write('**' '\\n')\n    output.write('*DISTRIBUTION TABLE, NAME=TABLE_MAT_ORIENTATION_1' '\\n')\n    output.write('COORD3D, COORD3D,' '\\n')\n    output.write('*DISTRIBUTION, LOCATION=ELEMENT, TABLE=TABLE_MAT_ORIENTATION_1, NAME=DISTRIBUTION_MAT_ORIENTATION_1' '\\n')\n    output.write('         ,                       1.,                       0.,                       0.,                       0.,                       1.,                       0.' '\\n')\n    for m in range(Max_no_El):\n        cogx_s=0\n        cogy_s=0\n        cogz_s=0\n        for n in range(No_Nodes_per_El):\n            cogx_sum[n]=nodes_arr[elements_arr[m,n+1]-1,1]+cogx_s\n            cogy_sum[n]=nodes_arr[elements_arr[m,n+1]-1,2]+cogy_s\n            cogz_sum[n]=nodes_arr[elements_arr[m,n+1]-1,3]+cogz_s\n            cogx_s=cogx_sum[n]\n            cogy_s=cogy_sum[n]\n            cogz_s=cogz_sum[n]\n        cogx[m]=cogx_s/No_Nodes_per_El\n        cogy[m]=cogy_s/No_Nodes_per_El\n        cogz[m]=cogz_s/No_Nodes_per_El\n\n        g1=int((cogx[m]-X0)/voxel_size)\n        g2=int((cogy[m]-Y0)/voxel_size)\n        g3=int((cogz[m]-Z0)/voxel_size)\n        if g1<vec.shape[3] and g1 >= 0 and g2<vec.shape[2] and g2 >= 0 and g3<vec.shape[1] and g3 >= 0:\n                \n                VecOrientX[m]=vec[2,g3,g2,g1]\n                VecOrientY[m]=vec[1,g3,g2,g1]\n                VecOrientZ[m]=vec[0,g3,g2,g1]\n                \n        else:\n                VecOrientX[m]=1\n                VecOrientY[m]=0\n                VecOrientZ[m]=0\n    m=0\n    for m in range(Max_no_El):\n        output.write(f'{m+1},        {VecOrientZ[m]:.4f},        {VecOrientY[m]:.4f},        {VecOrientX[m]:.4f},                       1.,                       0.,                       0.' '\\n')\n\n        if abs(VecOrientX[m]) > 0.9:\n            el_uni.append(m+1)\n        elif abs(VecOrientX[m]) < 0.1 and abs(VecOrientX[m]) >0.00000001:\n            el_backing.append(m+1)\n        elif abs(VecOrientX[m]) > 0.1 and abs(VecOrientX[m]) < 0.9:\n            el_biax.append(m+1)\n#       \n#            \n    output.write('*ORIENTATION, NAME=DISTRIBUTION_MAT_ORIENTATION_1, DEFINITION=COORDINATES, SYSTEM=RECTANGULAR' '\\n')\n    output.write('DISTRIBUTION_MAT_ORIENTATION_1,' '\\n')\n    output.write('       3,                       0.' '\\n')\n    output.write('*ELSET, ELSET=El_UNI' '\\n')\n    l=0\n    n=0\n    p=0\n    k=1\n    for l in range(len(el_uni)):\n        \n        if k==10:\n            output.write(f' {el_uni[l]},' '\\n')\n            k=1\n        else:\n            output.write(f' {el_uni[l]},') \n            k=k+1\n            \n    output.write('\\n')\n    \n    output.write('*ELSET, ELSET=El_Backing' '\\n')\n    m=1\n    for n in range(len(el_backing)):\n        \n        if m==10:\n            output.write(f' {el_backing[n]},' '\\n')\n            m=1\n        else:\n            output.write(f' {el_backing[n]},') \n            m=m+1\n\n    output.write('\\n')\n            \n    output.write('*ELSET, ELSET=El_Biax' '\\n')\n    o=1\n    for p in range(len(el_biax)):\n        \n        if o==10:\n            output.write(f' {el_biax[p]},' '\\n')\n            o=1\n        else:\n            output.write(f' {el_biax[p]},') \n            o=o+1\n#"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}